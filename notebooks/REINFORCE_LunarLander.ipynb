{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import time\n",
    "import random\n",
    "import gym\n",
    "from gym import envs\n",
    "\n",
    "env = gym.make('LunarLander-v2') # Create environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.004814433865249157, 1.4131004810333252, 0.4876282811164856, 0.09689344465732574, -0.005571866873651743, -0.11045505851507187, 0.0, 0.0]\n"
     ]
    }
   ],
   "source": [
    "s = env.reset()\n",
    "print(s.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Agent(nn.Module): \n",
    "    \n",
    "    def __init__(self, num_hidden = 128): \n",
    "        super(Agent, self).__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(8, num_hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(num_hidden, num_hidden), \n",
    "            nn.ReLU(),\n",
    "            nn.Linear(num_hidden, num_hidden), \n",
    "            nn.ReLU(),\n",
    "            nn.Linear(num_hidden, 4)\n",
    "        )        \n",
    "        \n",
    "    def forward(self, data): \n",
    "        x = self.layers(data)\n",
    "        return F.softmax(x, dim=1)\n",
    "    \n",
    "model = Agent()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_returns(rewards, discount_factor):\n",
    "    \"\"\"Compute discounted returns.\"\"\"\n",
    "    returns = np.zeros(len(rewards))\n",
    "    returns[-1] = rewards[-1]\n",
    "    for t in reversed(range(len(rewards)-1)):\n",
    "        returns[t] = rewards[t] + discount_factor * returns[t+1]\n",
    "    return returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-45-2eb23f7f881a>:39: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  sars = np.array(sars)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: -3724.0009765625\n",
      "Return: -10.450081700628214\n",
      "Loss: -2842.703125\n",
      "Return: -0.6234806969645783\n",
      "Loss: -642.762939453125\n",
      "Return: -12.352992574969363\n",
      "Loss: -1775.335693359375\n",
      "Return: -13.886263439497831\n",
      "Loss: -0.30898886919021606\n",
      "Return: -34.22972983993885\n",
      "Loss: -0.3366062343120575\n",
      "Return: -10.524194333347475\n",
      "Loss: -0.009178798645734787\n",
      "Return: -37.66151419775379\n",
      "Loss: -0.1919192671775818\n",
      "Return: -27.155809618051112\n",
      "Loss: -0.2696805000305176\n",
      "Return: -4.321965204451993\n",
      "Loss: -0.0005316638271324337\n",
      "Return: -28.94929764697186\n",
      "Loss: -0.0002367492124903947\n",
      "Return: -26.48529659773027\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-45-2eb23f7f881a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[0;31m#print(actions)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0;31m#print(model(torch.tensor(s, dtype=torch.float)).gather(1, torch.from_numpy(actions)).view(-1))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m \u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mREINFORCE\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-45-2eb23f7f881a>\u001b[0m in \u001b[0;36mREINFORCE\u001b[0;34m(episodes)\u001b[0m\n\u001b[1;32m     24\u001b[0m             \u001b[0;31m#print(logs)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m             \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muniform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m             \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmultinomial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;31m#                 a = env.action_space.sample()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "env = gym.make('LunarLander-v2') \n",
    "def REINFORCE(episodes=5000): \n",
    "    \n",
    "    def compute_loss(a_probs, returns):\n",
    "        return -torch.mean(torch.matmul(torch.log(a_probs), torch.from_numpy(returns).float()))\n",
    "    \n",
    "    # First sample a lot of episode\n",
    "    max_length_episode = 1001\n",
    "    \n",
    "    gamma = 0.9\n",
    "    epsilon = 0.1\n",
    "    \n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr = 0.005)\n",
    "    \n",
    "    losses= list()\n",
    "    total_ret = list()\n",
    "    for i in range(episodes):\n",
    "        #epsilon = epsilon * 0.95\n",
    "        s = env.reset()\n",
    "        sars = list()\n",
    "        optimizer.zero_grad()\n",
    "        for j in range(max_length_episode): \n",
    "            logs = model(torch.tensor([s], dtype=torch.float))\n",
    "            #print(logs)\n",
    "            r = random.uniform(0,1)\n",
    "            a = torch.multinomial(logs, 1).item()\n",
    "            if (r < epsilon):\n",
    "#                 a = env.action_space.sample()\n",
    "                s1, r, done, _ = env.step(0)\n",
    "                s = s1\n",
    "                continue\n",
    "            s1, r, done, _ = env.step(a)\n",
    "            sars.append((s.tolist(), a, r))\n",
    "            #print(sars[-1])\n",
    "            s = s1\n",
    "            if done: \n",
    "                break\n",
    "        #print(f\"Episode: {i}: sars {sars}\")\n",
    "        sars = np.array(sars)\n",
    "        states = np.vstack(sars[:, 0])\n",
    "        actions = np.vstack(sars[:, 1])     \n",
    "        rewards = np.array(sars[:,2], dtype=float)\n",
    "        returns = compute_returns(rewards, gamma)\n",
    "        a_probs = model(torch.from_numpy(states).float()).gather(1, torch.from_numpy(actions)).view(-1)\n",
    "        loss = compute_loss(a_probs, returns)\n",
    "        #print(loss)\n",
    "        losses.append(loss.item())\n",
    "        total_ret.append(returns[0])\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "            \n",
    "        if i % 100 == 0:\n",
    "            #print(f\"Epsilon: {epsilon}\")\n",
    "            print(f\"Loss: {loss.item()}\")\n",
    "            print(f\"Return: {returns[0]}\")\n",
    "    \n",
    "    return total_ret, losses\n",
    "        #print(actions)\n",
    "        #print(model(torch.tensor(s, dtype=torch.float)).gather(1, torch.from_numpy(actions)).view(-1))\n",
    "r, l = REINFORCE()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = env.reset()\n",
    "\n",
    "try:\n",
    "    for _ in range(50000):\n",
    "        env.render()\n",
    "        time.sleep(.01)\n",
    "        a = model(torch.from_numpy(np.atleast_2d(s)).float()).argmax().item()\n",
    "        s, r, done, _ = env.step(a)\n",
    "        if done:\n",
    "            s = env.reset()\n",
    "except KeyboardInterrupt:\n",
    "    env.close()\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
